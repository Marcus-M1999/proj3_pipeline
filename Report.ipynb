{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c83802",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0dcaa",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cfebd",
   "metadata": {},
   "source": [
    "### Terminal Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd262e",
   "metadata": {},
   "source": [
    "`cp ~/w205/course-content/12a/docker-compose.yml .`\n",
    "Copying the docker-compose.yml file to this folder \n",
    "\n",
    "\n",
    "`docker-compose up -d`\n",
    "Spinning up docker in the background\n",
    "\n",
    "\n",
    "`docker-compose exec kafka kafka-topics --create --topic MessageHistory --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181`\n",
    "Created MessageHistory topic to be used in this project. \n",
    "\n",
    "\n",
    "`docker-compose exec mids env FLASK_APP=/w205/project-3-Marcus-M1999/game_api.py flask run --host 0.0.0.0\"`\n",
    "Spinning up the flask server to take in the data sent to it with \"GET\" requests. Flask is an API that is used to build webservers, in this case it is used to create an instance of the host server to take in data being sent to it. \n",
    "\n",
    "\n",
    "`docker-compose exec mids kafkacat -C -b kafka:29092 -t MessageHistory -o beginning`\n",
    "Starts Kafkacat listening to the MessageHistory topic, so I can see the what data is flowing through it. \n",
    "\n",
    "`Sh ab.sh`\n",
    "This command runs the ab shell script below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465f927",
   "metadata": {},
   "source": [
    "### ab.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362127e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-17eaa80d68eb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-17eaa80d68eb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    while true;\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "while true;\n",
    "do\n",
    "docker-compose exec mids ab -n 10 -H \"Host: test_connection.game.com\" http://localhost:5000/\n",
    "docker-compose exec mids \\\n",
    "    ab -n 10 -H \"Host: requester.game.com\" \\\n",
    "      http://localhost:5000/request_group\n",
    "  sleep 10;\n",
    "  docker-compose exec mids \\\n",
    "    ab -n 10 -H \"Host: acceptor.game.com\" \\\n",
    "      http://localhost:5000/accept_member\n",
    "  sleep 10;\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346797c",
   "metadata": {},
   "source": [
    "This script contains an infinite while loop used to generate test data using apache bench; a testing tool created by apache, with the data from the game_api.py file. The while ‘-n 10’ part means that it is sending 10 commands, and the -H sets the host to requester.game.com or acceptor.game.com. Finally there is a 10 second break between each request and acceptance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c34e75",
   "metadata": {},
   "source": [
    "### write_requestor_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Extract events from kafka and write them to hdfs\n",
    "\"\"\"\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def requestor_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_requestor_event(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'request_group':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"main\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"MessageHistory\") \\\n",
    "        .load()\n",
    "\n",
    "    requestors = raw_events \\\n",
    "        .filter(is_requestor_event(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          requestor_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "    spark.sql(\"drop table if exists requestors\")\n",
    "    sql_string = \"\"\"\n",
    "        create external table if not exists requestors (\n",
    "            raw_event string,\n",
    "            timestamp string,\n",
    "            Accept string,\n",
    "            Host string,\n",
    "            `User-Agent` string,\n",
    "            event_type string\n",
    "            )\n",
    "            stored as parquet\n",
    "            location '/tmp/requestors'\n",
    "            tblproperties (\"parquet.compress\"=\"SNAPPY\")\n",
    "            \"\"\"\n",
    "    spark.sql(sql_string)\n",
    "\n",
    "    sink = requestors \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_requestors\") \\\n",
    "        .option(\"path\", \"/tmp/requestors\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    sink.awaitTermination()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226678d",
   "metadata": {},
   "source": [
    "\n",
    "The requestor_event_schema() defines the schema for the requestor event. In this case I did not change it from the schema originally in the file, but it is possible to add other fields concerning the metadata, or nested fields within the JSON data. <br>\n",
    "The is_requestor_event() method takes in the event in the form of JSON data, and filters it out to see if it’s a ‘request_group’ event type in order to see if it belongs in this table or a different one. <br>\n",
    "The main() method creates a spark session with hive support, and creates the raw_events which are the events received from subscribing to the MessageHistory topic on the Kafka server.<br>\n",
    "The requestors object casts the raw_events as strings so it can be queryed in Presto. <br>\n",
    "The spark.sql(\"drop table if exists requestors\") deletes the table from hdfs if it already exists. This is done to avoid errors that could arise if data was already written to the table and then overwritten (assuming the table was already created). <br>\n",
    "The sql_string portion of the file contains the query to create the requestors table, and store it in the parquet format in hdfs. <br>\n",
    "The spark.sql(sql_string) runs the sql query, creating the new table. <br>\n",
    "The sink object writes the new table to hdfs, and saves it under the /tmp/requestors directory. <br>\n",
    "The processingTime=”10 seconds” waits 10 seconds before writing the data again. The 10 seconds was kept from the original file, and was not changed since it was an appropriate amount of time to wait while I was testing this pipeline. <br>\n",
    "The sink.awaitanyTermination() method allows the sink to run continuously, until the keyboard interrupt (CTRL + C) is hit. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c4705",
   "metadata": {},
   "source": [
    "### Terminal Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9225f",
   "metadata": {},
   "source": [
    "`docker-compose exec spark spark-submit /w205/project-3-Marcus-M1999/write_requestor_stream.py`\n",
    "This command runs a spark script; write_requestor_stream.py,\n",
    "\n",
    "`docker-compose exec spark spark-submit /w205/project-3-Marcus-M1999/write_acceptor_stream.py`\n",
    "This commmand runs a similar script, but for the acceptor events rather than the requestor events. \n",
    "\n",
    "`docker-compose exec cloudera hadoop fs -ls /tmp/`\n",
    "This checks the cloudera server to make sure that the data for both requestors and acceptors is present and the scripts are working as intended. \n",
    "\n",
    "`docker-compose exec presto presto --server presto:8080 --catalog hive --schema default`\n",
    "This command begins to run presto, and uses hive to get the list of queryable tables in hdfs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3505796",
   "metadata": {},
   "source": [
    "### game_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event.update(request.headers)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def default_response():\n",
    "    default_event = {'event_type': 'default'}\n",
    "    log_to_kafka('events', default_event)\n",
    "    return \"This is the default response!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/request_group\")\n",
    "def request_friend():\n",
    "    add_event = {'event_type': 'request_group'}\n",
    "    log_to_kafka('MessageHistory', add_event)\n",
    "    return \"Request Submitted!\\n\"\n",
    "\n",
    "@app.route(\"/accept_member\")\n",
    "def accept_member():\n",
    "    accept_event = {'event_type': 'accept_member'}\n",
    "    log_to_kafka('MessageHistory', accept_event)\n",
    "    return \"Member Approved!\\n\"\n",
    "\n",
    "@app.route(\"/decline_member\")\n",
    "def decline_member():\n",
    "    accept_event = {'event_type': 'decline_member'}\n",
    "    log_to_kafka('MessageHistory', decline_event)\n",
    "    return \"Member Declined!\\n\"\n",
    "\n",
    "@app.route(\"/message\")\n",
    "def message():\n",
    "    message_event = {'event_type': 'message'}\n",
    "    log_to_kafka('MessageHistory', message_event)\n",
    "    return \"Message sent!\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf27f0",
   "metadata": {},
   "source": [
    "This is the game api file, that contains the api data that would be received from the game/platform. \n",
    "Some of the commands in this file include a default event, request_group, and accept_member among others.\n",
    "Each method adds the event to the kafka topic listed and returns a string to confirm it executed correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cbb69",
   "metadata": {},
   "source": [
    "### docker-compose.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f48840",
   "metadata": {},
   "source": [
    "The docker-compose.yml file contains the configuration for docker to run off, and all of the services as well as ports needed to run the services. From top to bottom the services include:\n",
    "\n",
    "##### 1.)  Zookeeper\n",
    "Zookeeper is the service used to manage all other services in the container. The image line refers to the version of the service being used in the container. The client port refers to the port in use to communicate with Kafka. The exposed, or open ports are listed under \"expose\" (for additional connections in the future).\n",
    "\n",
    "##### 2.) Kafka \n",
    "Kafka is the second service in the docker configuration file and has several new lines including depends_on. The depends_on keyword means that docker will first spin up Zookeeper and then Kafka since Kafka uses Zookeeper to manage all other services. Under the environment variables, `KAFKA_BROKER_ID` is assigned to 1, since it's the first and only node in the cluster. (The Broker ID identifies each broker in the cluster with a unique identifier, 1 in this case.) `KAFKA_ZOOKEEPER_CONNECT` displays the port that zookeeper is connected to. `KAFKA_ADVERTISED_LISTENERS` lists all listeners that the broker (Kafka node) will display to all producers and consumers of records in the pipeline. The port connected, `kafka:29092` refers to the Kafka port that Kafka connects to communicate with Zookeeper. `KAFKA_OFFSETS_TOPIC_REPLICATOIN_FACTOR` refers to the number of replications that occur with the data between multiple brokers. In this case, the replication factor is 1 since there is only 1 broker. \n",
    "\n",
    "##### 3.) Cloudera\n",
    "Cloudera is a hosting service for Hadoop and contains a variety of additional tools on top of Apache's version of Hadoop (Apache is the creator of Hadoop). \n",
    "\n",
    "##### 4.) Spark\n",
    "Spark is the service used to process messages and relay them to the hdfs nodes ran by Cloudera. `stdin_open` and `TTY keep the container running in a \"detached mode\" allowing the service to persist in an interactive virtual environment. The `volumes` connect the directory ~/w205/ to /w205, to read and write files in spark. The `environment lists the environment variables including the `Hadoops_NameNode` cluster that spark sends the processed data to. \n",
    "\n",
    "##### 5.) Presto\n",
    "Presto is a querying tool used to pull data from big data stores such as Hadoop File System (hdfs). In this context Presto is connected to a Hive Thrift Server, or a server that Hive runs to track the available tables. This is how Presto can view the tables in hdfs. \n",
    "\n",
    "##### 6.) mids\n",
    "mids is the final container in the docker-compose file that holds tools such as Kafkacat (a tool used to produce and consume messages with Kafka). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c832b02",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176aa752",
   "metadata": {},
   "source": [
    "##### 1.) How many requests to join a group are there?\n",
    "`SELECT COUNT(*) FROM requestors;`\n",
    "This query shows the count of all requests to join a group. <br>\n",
    "Output:\n",
    "\n",
    "\n",
    "`\n",
    " _col0 \n",
    "-------\n",
    "    80 \n",
    "(1 row)\n",
    "\n",
    "`\n",
    "\n",
    "##### 2.) How many of those requests were accepted?\n",
    "`SELECT COUNT(*) FROM acceptor;`\n",
    "This query is similar to the above one shows the count of all acceptances to join a group. <br>\n",
    "Output:\n",
    "\n",
    "`\n",
    " _col0 \n",
    "-------\n",
    "   180 \n",
    "(1 row)\n",
    "\n",
    "`\n",
    "<br>\n",
    "Note that the acceptances are higher than the requests because the script; writing_acceptors.py, was started later and ran longer than the writing_requestors.py script. \n",
    "<br>\n",
    "##### 3.) Who were the hosts for the requestor events?\n",
    "`SELECT host, event_type FROM requestors;`\n",
    "In the testing data there was only one host, however, in a real world scenario the hosts would be different and could offer some insights to the players. <br>\n",
    "Output:<br>\n",
    "`        host        |  event_type   \n",
    "--------------------+---------------\n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    " requester.game.com | request_group \n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeeb5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
